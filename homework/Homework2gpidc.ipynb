{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 8 Homework 2 - Visualisation and Regression\n",
    "\n",
    "## Homework - Due Friday 30th June\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account\n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). \n",
    "    * What have we covered so far from this paper? \n",
    "    * Explain sections 6-13 in your own words\n",
    "\n",
    "#### Machine Learning\n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "* Complete the first 3 exercises from Chapter 3 of Introduction to Statistical Learning in Python\n",
    "\n",
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Alasdair and Ian over Slack.\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "* Draft/Sketch (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it. Try it out if you get a chance.\n",
    "\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework2_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "#### Sign up for an AWS Account\n",
    "https://us-east-2.console.aws.amazon.com/console/home?region=us-east-2#\n",
    "Account id: 511004532925\n",
    "\n",
    "#### Sign up for a Google Cloud Account\n",
    "https://console.cloud.google.com/home/dashboard?project=vernal-buffer-172320\n",
    "Project ID: vernal-buffer-172320\n",
    "No. 1079400889046"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plain English Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Take a series of measurements for which we want to make a prediction on.\n",
    "\n",
    "A good example might be 'blood pressure' and 'age'.\n",
    "\n",
    "Both of them are \"continuous\", or numbers that can have any value from minus infinity to infinity.\n",
    "One of these numbers is the 'target' we want to predict, and the other is a 'feature' we will use to make our prediction. \n",
    "\n",
    "In the example, the target is **blood pressure**, and the feature is **age**.\n",
    "\n",
    "A fast, easy, and highly interpretable method we can use to make this prediction is called **'linear regression'**. \n",
    "\n",
    "Linear regression, in a nut shell, assumes that a prediction can be made through 'fitting' a straight line through the points, and using the 'coefficients' of this line, like the gradient and the intercept, to predict the target 'y' for any set of n features 'x1, x2, ..., xn'\n",
    "\n",
    "i.e.\n",
    "$y = \\beta_0 + \\beta_1x$\n",
    "\n",
    "Where\n",
    "- $y$ is the response\n",
    "- $x$ is the feature\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for x\n",
    "\n",
    "Now, for a lot of people first learn how to do this through 'visual inspection', where they plot the points and estimate the 'line of best fit'. \n",
    "\n",
    "e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup packages\n",
    "import pandas as pd\n",
    "# ! pip install seaborn\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import some data\n",
    "Age_BloodPressure = pd.read_csv('Age_BloodPressure.csv',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x117b14c88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEFCAYAAAABjYvXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117bb0dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the Data\n",
    "# X = Age_BloodPressure['Age']\n",
    "# Y = Age_BloodPressure['Systolic Blood Pressure']\n",
    "sns.regplot(x='Age', y=\"Systolic Blood Pressure\", data=Age_BloodPressure, fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimate a line that will cross these points. \n",
    "Intercept = 100\n",
    "Gradient = 10 blood pressure units/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading: Useful things to know about machine learning\n",
    "**What have we covered?**\n",
    "Most concepts of sections 1 and 2, arranged in a novel taxonomy of Table 1. <br>Great reinforcement of \"Generalization\" being the end goal in section 3, and holdouts being mandatory (hope I'm never in the situation of using test data to tune an algorithm).<br>\n",
    "Some really novel abstractions in section 4 - like relating this all back to induction, and how some assumptions (like smoothing) in practice can act as a lever transforming a small amount of input knowledge into a large amount of output knowledge... <br>\n",
    "\n",
    "QUOTE?  \"Programming, like all engineering, is a lot of work:\n",
    "we have to build everything from scratch. Learning is more\n",
    "like farming, which lets nature do most of the work. Farmers\n",
    "combine seeds with nutrients to grow crops. Learners\n",
    "combine knowledge with data to grow programs\"\n",
    "\n",
    "We have also covered overfitting and the bias variance trade-off - \n",
    "Overfitting as hallucination - \"encoding random quirks of the data\".\n",
    "Bias - Tendancy to consistently learn the same wrong thing.\n",
    "Variance - Tendency to learn random things irrespective of the signal.\n",
    "No free lunch - easy to avoid overfitting (variance) by falling into the opposite error of underfitting (bias). Simultaneously avoiding both requires a perfect classifier - short of knowing in advance, there is no single technique that will always do best (so no free lunch).\n",
    "\n",
    "False discovery rate - multiple testing - worth exploring further (along with concepts of power).\n",
    "\n",
    "**Section 6 Summary**\n",
    "*Intuition fails in high dimensions* <br> \n",
    "A large number of features introduce the following challenges to modelling:\n",
    "1. as the number of features increase, the less input space a given training will cover, and the more the model will need to generalize. \n",
    "2. The more features there are, the more likely there are irrelevant features that introduce noise to the model. \n",
    "3. The more features there are, the more alike a sample looks like (choice of nearest neighbours becomes random). \n",
    "4. Our intuitions apply to three dimensions, and we are lost in high dimensions.\n",
    "Some of these issues can be counteracted by non-uniformity in a sample's distribution, or through using processing techniques that reduce the number of dimensions (i.e. PCA).\n",
    "The take home is that adding new features is 'not harmless' and can introduce real complications in machine learning.\n",
    "\n",
    "**Section 7 Summary**\n",
    "*Theoretical guarantees are not what they seem*\n",
    "Research is full of guarantees, like the number of samples needed for generalization. \n",
    "But machine learning is induction, and historically induction has been known as a far riskier way to knowledge than other reasoning methods like deduction.\n",
    "Furthermore these guarantees at times argue for more samples than needed in practice.\n",
    "So these guarantees are not useful practically - they act more as tools for research.\n",
    "<color=red> Buyer beware: Just because a learner has a theoretical justification and works in practice doesn't mean the theory is important for the practice </color>\n",
    "\n",
    "**Section 8 Summary**\n",
    "*Feature Engineering is the Key*\n",
    "The most important difference between a successful and failed ML project is the features used.\n",
    "1. Most effort in a typical project goes into data wrangling - it's the bit that's domain specific, and hasn't been engineered (unlike the learners).\n",
    "2. ML tends to be an interative process of running the learning, analyzing the results, modifying the data and the learner, and repeating. \n",
    "3. Features tend to be domain specific, may be material in combination, and may introduce resource cost or risks of overfitting.\n",
    "\n",
    "\n",
    "**Section 9 Summary**\n",
    "*More Data beats a cleverer algorithm*\n",
    "Machine learning is all about letting data do the heavy lifting in designing an algorithm.\n",
    "The rule of thumb is - a dumb algorithm with lots of data will beat a clever algorithm with modest amounts of it. So try the simplest learners first (naive Bayes before logistic, k-nearest neighbor before support vector machines)\n",
    "Machine Learning has three limited resources to juggle - time, memory, and training data. Today, time is the bottle-neck - mountains of data are available, but there isn't enough time to process it, so it goes unused. And in terms of time, HUman Cycles, not CPU Cycles, is the key bottle-neck. Complex classifiers can take too long to learn.\n",
    "\"All learners essentially work by grouping nearby examples into the same class - the key difference is NEARBY\".\n",
    "\n",
    "Human effort saved and insight gained...\n",
    "\n",
    "**Section 10 Summary**\n",
    "*Learn many models - not just one*\n",
    "Old models used to involve one learner. THese days models involve ensembles.\n",
    "Reeduce the variance with a slight increase in bias.\n",
    "Bagging - combine the results and pick by voting.\n",
    "Boosting - training examples have weights which are varied so that each new classifier focuses on the examples that the previous ones tended to get wrong.\n",
    "Winner and runner up of Netflix prize had ensembles of over 100 learners.\n",
    "\n",
    "**Section 11 Summary**\n",
    "*Simplicity does not imply accuracy*\n",
    "Given 2 models of similar training accuracy, the simpler of the two will not have lower test accuracy - there is no free lunch.\n",
    "\n",
    "**Section 12 Summary**\n",
    "*Representable does not imply learnable*\n",
    "Lots of jargon here - representation roughly matches the features, target, and algorithm applied to the data.\n",
    "This section may be trying to warn against becoming too attached to one representation (i.e. always using random forests).\n",
    "\n",
    "**Section 13 Summary**\n",
    "*Correlation does not imply causation*\n",
    "Yep. Suppose it needed a mention for compleness.\n",
    "Nice distinction raised - most learners are applied to observational data, where the variables are not under control by the learner, unlike experimental data.\n",
    "So the outcome is always a correlation, that needs further investigation or experimentation to confirm into a causation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "#### Three ways to select features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three ways to select features include: <br>\n",
    "    1. Applying your deep knowledge of the problem domain \n",
    "    2. Selecting features based on Chi squared/information gain/correlation coefficients\n",
    "    3. Using regularization methods that bias the model to less features/complexity during its construction\n",
    "        a. LASSO\n",
    "        b. Elastic Net\n",
    "        c. Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ISLR Chapter 3 Exercise 1-3 in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions\n",
    "1. Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.\n",
    "2. Carefully explain the differences between the KNN classifier and KNN regression methods.\n",
    "3. Suppose we have a data set with five predictors, X1 = GPA, X2 = IQ, X3 = Gender (1 for Female and 0 for Male), X4 = Interaction between GPA and IQ, and X5 = Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get βˆ0 = 50, βˆ1 = 2 0 , βˆ 2 = 0 . 0 7 , βˆ 3 = 3 5 , βˆ 4 = 0 . 0 1 , βˆ 5 = − 1 0 . <br>\n",
    "a) Which answer is correct, and why? <br>\n",
    "    i. For a fixed value of IQ and GPA ,males earn more on average than females <br>\n",
    "    ii. For a fixed value of IQ and GPA, females earn more on average than males. <br>\n",
    "    iii. ForafixedvalueofIQandGPA,malesearnmoreonaverage than females provided that the GPA is high enough. <br>\n",
    "    iv. For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough. <br>\n",
    "(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0. <br>\n",
    "(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "**Course Project**\n",
    "For the following setup a new github repository for your project and share it with Alasdair and Ian over Slack.\n",
    "Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "Draft/Sketch (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it. Try it out if you get a chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Github Repo Details\n",
    "github.com/GeoffPidcock/Tradeshow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
