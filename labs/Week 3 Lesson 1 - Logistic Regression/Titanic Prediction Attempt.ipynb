{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Prediction Attempt\n",
    "## Geoff Pidcock | June 17th 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and wrangling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
       "       'Ticket', 'Fare', 'Cabin', 'Embarked'], dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and wrangle data\n",
    "import pandas as pd\n",
    "titanic = pd.read_csv('train.csv',index_col='PassengerId')\n",
    "# titanic.isnull().sum() # Age has 177 null values, Cabin 687, Embarked 2\n",
    "titanic.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q', nan], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at values and distributions of categorical features\n",
    "# titanic.groupby('Pclass')['Name'].count() # values 1,2,3\n",
    "# titanic.groupby('SibSp')['Name'].count() # values 0 to 8\n",
    "# titanic.groupby('Parch')['Name'].count() # values 0 to 6\n",
    "# titanic.groupby('Sex')['Name'].count() # values female and male\n",
    "titanic.Embarked.unique()\n",
    "# Age, Fare, continuous variables.\n",
    "# titanic.Cabin.unique().shape #148 unique values for cabin. 687 nulls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Survived', 'Name', 'Age', 'Ticket', 'Fare', 'Cabin', 'Sex_female',\n",
       "       'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Pclass_1',\n",
       "       'Pclass_2', 'Pclass_3', 'Parch_0', 'Parch_1', 'Parch_2', 'Parch_3',\n",
       "       'Parch_4', 'Parch_5', 'Parch_6', 'SibSp_0', 'SibSp_1', 'SibSp_2',\n",
       "       'SibSp_3', 'SibSp_4', 'SibSp_5', 'SibSp_8'], dtype=object)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hypothesis - siblings and spouses, parents and children, gender, class,\n",
    "# age should hold information towards survival.\n",
    "\n",
    "# Assign dummies for Pclass, SibSp, Parch, Sex, Embarked\n",
    "\n",
    "titanic_wdum = pd.get_dummies(data=titanic, columns = ['Sex', 'Embarked', 'Pclass', 'Parch', 'SibSp'], prefix = ['Sex', 'Embarked', 'Pclass', 'Parch', 'SibSp'] )\n",
    "titanic_wdum.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 177\n"
     ]
    }
   ],
   "source": [
    "# Fill 177 null values for Age\n",
    "\n",
    "# titanic_wdum['Age'] = titanic_wdum[['Age','Parch_0','Parch_1','Parch_2','Parch_3','Parch_4','Parch_5','Sex_male','Pclass_1', 'Pclass_2','Embarked_C','Embarked_Q','SibSp_0','SibSp_1','SibSp_2','SibSp_3','SibSp_4','SibSp_5']].groupby(['Age','Parch_0','Parch_1','Parch_2','Parch_3','Parch_4','Parch_5','Sex_male','Pclass_1', 'Pclass_2','Embarked_C','Embarked_Q','SibSp_0','SibSp_1','SibSp_2','SibSp_3','SibSp_4','SibSp_5'])['Age'].transform(lambda x: x.fillna(x.mean()))\n",
    "# That doesn't seem to work - too much grouping in applying the averages?\n",
    "# Another alternative - just fill with global average?\n",
    "titanic_wdum.Age.fillna(titanic.Age.mean(), inplace=True)\n",
    "print(titanic_wdum.Age.isnull().sum(),titanic.Age.isnull().sum())\n",
    "\n",
    "# If needed - can explore imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel\n",
    "# May need to do this in advance of model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trying Regularized logistic regression w/ CV\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "logregcv = LogisticRegressionCV(Cs = [1e-4,1e-2,1e0,1e2,1e4], cv = 5, penalty='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseEstimator.get_params of LogisticRegressionCV(Cs=[0.0001, 0.01, 1.0, 100.0, 10000.0],\n",
      "           class_weight=None, cv=5, dual=False, fit_intercept=True,\n",
      "           intercept_scaling=1.0, max_iter=100, multi_class='ovr',\n",
      "           n_jobs=1, penalty='l2', random_state=None, refit=True,\n",
      "           scoring=None, solver='lbfgs', tol=0.0001, verbose=0)> [[ 0.88453239  0.11546761]\n",
      " [ 0.0700559   0.9299441 ]\n",
      " [ 0.40180375  0.59819625]\n",
      " ..., \n",
      " [ 0.38842028  0.61157972]\n",
      " [ 0.43382357  0.56617643]\n",
      " [ 0.88676966  0.11323034]]\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['Age', 'Sex_female', 'Embarked_C', 'Embarked_Q', 'Pclass_1', 'Pclass_2', 'Parch_0', 'Parch_1', 'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'SibSp_0', 'SibSp_1', 'SibSp_2', 'SibSp_3', 'SibSp_4', 'SibSp_5']\n",
    "X = titanic_wdum[feature_cols]\n",
    "y = titanic.Survived\n",
    "# X.to_csv('titanicX_1.csv')\n",
    "logregcv.fit(X,y)\n",
    "print(logregcv.get_params,logregcv.predict_proba(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Age', -0.037615544514703961),\n",
       " ('Sex_female', 2.677718059360441),\n",
       " ('Embarked_C', 0.33836153468423374),\n",
       " ('Embarked_Q', 0.44729514323719277),\n",
       " ('Pclass_1', 2.2076692107010008),\n",
       " ('Pclass_2', 1.0887160326083032),\n",
       " ('Parch_0', 1.0068928576521314),\n",
       " ('Parch_1', 1.419981539598175),\n",
       " ('Parch_2', 1.1088077162791075),\n",
       " ('Parch_3', 1.3239120602565499),\n",
       " ('Parch_4', -2.8469557722627417),\n",
       " ('Parch_5', -0.095248345247983424),\n",
       " ('SibSp_0', 2.4452779448352091),\n",
       " ('SibSp_1', 2.5385096820034505),\n",
       " ('SibSp_2', 2.1777766056498957),\n",
       " ('SibSp_3', 0.3059722653788029),\n",
       " ('SibSp_4', 0.69312049871183767),\n",
       " ('SibSp_5', -2.1033212920774571)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No idea which features are performing the best in this prediction.\n",
    "# probs=logregcv.predict_proba(X) # sample level probabilities\n",
    "# probs.shape\n",
    "feature_coef=zip(feature_cols,logregcv.coef_[0])\n",
    "list(feature_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81481481481481477"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logregcv.score(X,y)\n",
    "# Aaargh - 81% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "titanictest = pd.read_csv('test.csv',index_col='PassengerId')\n",
    "titanictest_wdum = pd.get_dummies(data=titanictest, columns = ['Sex', 'Embarked', 'Pclass', 'Parch', 'SibSp'], prefix = ['Sex', 'Embarked', 'Pclass', 'Parch', 'SibSp'] )\n",
    "titanictest_wdum.Age.fillna(titanictest.Age.mean(), inplace=True)\n",
    "feature_cols = ['Age', 'Sex_female', 'Embarked_C', 'Embarked_Q', 'Pclass_1', 'Pclass_2', 'Parch_0', 'Parch_1', 'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'SibSp_0', 'SibSp_1', 'SibSp_2', 'SibSp_3', 'SibSp_4', 'SibSp_5']\n",
    "Xpred = titanictest_wdum[feature_cols]\n",
    "titanictest_wdum['Survived'] = logregcv.predict(Xpred)\n",
    "# ypred.to_csv('test_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Export predictions as a csv\n",
    "titanictest_wdum['Survived'].to_csv('testpred.csv',header=True,index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit to Kaggle and review according to the score**\n",
    "https://www.kaggle.com/c/titanic/leaderboard\n",
    "## Result: 0.77033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression reference - from lab 4\n",
    "# Select feature columns\n",
    "feature_cols = ['Pclass', 'Parch', 'Age']\n",
    "X = titanic[feature_cols]\n",
    "# Training/testing data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "zip(feature_cols, logreg.coef_[0])\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Other models to look at:\n",
    "Logistic Regression, \n",
    "Support Vector Machines, \n",
    "Neural Networks, or \n",
    "Decision Tree-based methods such as Gradient Boosted Decision Trees (GBDT)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy Evaluation reference\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn import metrics\n",
    "prds = logreg.predict(X)\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation reference - Week 3 Lab_B\n",
    "from sklearn import cross_validation\n",
    "X = bikeshare[['temp', 'hum']].join(weather[['weather_1', 'weather_2', 'weather_3']])\n",
    "y = bikeshare.casual \n",
    "kf = cross_validation.KFold(len(X), n_folds=5, shuffle=True)\n",
    "mse_values = []\n",
    "scores = []\n",
    "n= 0\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
    "for train_index, test_index in kf:\n",
    "    lm = linear_model.LinearRegression().fit(modeldata.iloc[train_index], y.iloc[train_index])\n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(modeldata.iloc[test_index])))\n",
    "    scores.append(lm.score(modeldata, y))\n",
    "    n+=1\n",
    "    print ('Model', n)\n",
    "    print ('MSE:', mse_values[n-1])\n",
    "    print ('R2:', scores[n-1])\n",
    "\n",
    "\n",
    "print (\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print ('Mean of MSE for all folds:', np.mean(mse_values))\n",
    "print ('Mean of R2 for all folds:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regularization reference - week 4 lab 1\n",
    "# Setting up a CV version of the elastic net model\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "# Specifying parameter ranges\n",
    "alpha_rangeEN = 10.**np.arange(-3, 3)\n",
    "# l1_ratio_rangeEN = 10.**np.arange(-6,1) \n",
    "l1_ratio_rangeEN = [.1, .5, .7, .9, .95, .99, 1] # provides a better result.\n",
    "enetcv = ElasticNetCV(alphas=alpha_rangeEN,l1_ratio=l1_ratio_rangeEN)\n",
    "print(enetcv.get_params())\n",
    "enetcv.fit(X_train,y_train)\n",
    "print('Optimal Alpha Value: ',enetcv.alpha_)\n",
    "print('Optimal l1_ratio Value: ',enetcv.l1_ratio_)\n",
    "preds3 = enetcv.predict(X_test)\n",
    "print('RMSE (enetcv reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
