{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - Regularization\n",
    "\n",
    "## Week 4 Tuesday 11th June\n",
    "\n",
    "### TASK: Regularized regression\n",
    "### FUNCTIONS: Ridge, RidgeCV, Lasso, LassoCV\n",
    "### DOCUMENTATION: http://scikit-learn.org/stable/modules/linear_model.html\n",
    "### DATA: Crime (n=319 non-null, p=122, type=regression)\n",
    "### DATA DICTIONARY: http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime\n",
    "\n",
    "This data set contains data on violent crimes within a community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319, 123)\n"
     ]
    }
   ],
   "source": [
    "########## Prepare data ##########\n",
    "# read in data, remove categorical features, remove rows with missing values\n",
    "import pandas as pd\n",
    "crime = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data', header=None, na_values=['?'])\n",
    "crime = crime.iloc[:, 5:] # Stripping out descriptive features like county\n",
    "crime.dropna(inplace=True) # Dropping out missing values.\n",
    "crime.head()\n",
    "print(crime.shape)\n",
    "\n",
    "# define X and y\n",
    "X = crime.iloc[:, :-1]\n",
    "y = crime.iloc[:, -1] # last column is goal variable - violent crimes per 100K\n",
    "\n",
    "# split into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 122)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many columns are in X?\n",
    "X.shape\n",
    "# 122 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, -3.6618816659782105),\n",
       " (6, 0.69812446462463851),\n",
       " (7, -0.2619554671596705),\n",
       " (8, -0.28527002715028194),\n",
       " (9, -0.16474083727537459),\n",
       " (10, 0.24697233272421826),\n",
       " (11, -1.0929005121936748),\n",
       " (12, -0.59685779593661115),\n",
       " (13, 1.1120023933804974),\n",
       " (14, -0.72196893103299542),\n",
       " (15, 4.273465976681706),\n",
       " (16, -0.22804026751251311),\n",
       " (17, 0.80487576889829482),\n",
       " (18, -0.25793473181730497),\n",
       " (19, -0.26345802313128558),\n",
       " (20, -1.0461695798507957),\n",
       " (21, 0.60778419661167216),\n",
       " (22, 0.7735525605510527),\n",
       " (23, 0.059646802945674343),\n",
       " (24, 0.69021592158790068),\n",
       " (25, 0.021675943000021583),\n",
       " (26, -0.48780294904572419),\n",
       " (27, -0.51885840432656016),\n",
       " (28, 0.13947881456806871),\n",
       " (29, -0.12441794219332974),\n",
       " (30, 0.31500382101797386),\n",
       " (31, -0.15263373607292369),\n",
       " (32, -0.96500392695265746),\n",
       " (33, 1.1714216270172748),\n",
       " (34, -0.030854669038923768),\n",
       " (35, -0.92908554819852307),\n",
       " (36, 0.1246545856596646),\n",
       " (37, 0.19810450590177639),\n",
       " (38, 0.73080482054293505),\n",
       " (39, -0.17733729365325046),\n",
       " (40, 0.083292758757580548),\n",
       " (41, 0.34604560069974888),\n",
       " (42, 0.50183733752944304),\n",
       " (43, 1.570629578550633),\n",
       " (44, -0.41347880687158411),\n",
       " (45, 1.3935080202150019),\n",
       " (46, -3.4942811412914097),\n",
       " (47, 0.70957781816024612),\n",
       " (48, -0.83214135200406536),\n",
       " (49, -1.3998492732504819),\n",
       " (50, 1.0248284011770838),\n",
       " (51, 0.21385500638475696),\n",
       " (52, -0.61893732486623387),\n",
       " (53, 0.52895449004846073),\n",
       " (54, 0.079829488988700237),\n",
       " (55, 0.059368856022228467),\n",
       " (56, -0.16858266675304262),\n",
       " (57, 0.73126405114905224),\n",
       " (58, -1.396352075998003),\n",
       " (59, 0.23850770386735654),\n",
       " (60, 0.55062143883521564),\n",
       " (61, -0.56144786716912853),\n",
       " (62, 0.61898976373519909),\n",
       " (63, 2.5551702422961799),\n",
       " (64, -3.7176959868014747),\n",
       " (65, 0.709191934627312),\n",
       " (66, 0.38204143861516304),\n",
       " (67, 0.82375283638813657),\n",
       " (68, -1.6770354661219025),\n",
       " (69, -1.7315044977394849),\n",
       " (70, 0.99012017126581764),\n",
       " (71, -0.57274569695217514),\n",
       " (72, -1.4587729489219743),\n",
       " (73, 0.86803214437696041),\n",
       " (74, 0.51595998416110334),\n",
       " (75, 0.03144532067921868),\n",
       " (76, 0.20186979124880933),\n",
       " (77, 0.096529193959183524),\n",
       " (78, 2.1303409930099968),\n",
       " (79, -0.069537442267433136),\n",
       " (80, 0.04624770228083408),\n",
       " (81, -0.011056595545127113),\n",
       " (82, -0.013431378034132579),\n",
       " (83, -0.10451549387050196),\n",
       " (84, -0.87698517103190077),\n",
       " (85, 0.42678190687907741),\n",
       " (86, -0.18540579521961909),\n",
       " (87, -0.81621551723963492),\n",
       " (88, -0.28659607574454155),\n",
       " (89, -0.15611070775753511),\n",
       " (90, 1.7646858047092076),\n",
       " (91, -0.57016372975916063),\n",
       " (92, -0.075406670447576513),\n",
       " (93, -0.17421269654207303),\n",
       " (94, -0.088974721985246807),\n",
       " (95, 0.22633640279020117),\n",
       " (96, 1.3803007280374391),\n",
       " (97, -0.33730474356188656),\n",
       " (98, -0.025785661113098934),\n",
       " (99, 0.08912991878680504),\n",
       " (100, 0.34987679337480909),\n",
       " (101, -1.2242855701883344),\n",
       " (102, -36.79412052869975),\n",
       " (103, -0.69569974967685377),\n",
       " (104, 0.29526927932010627),\n",
       " (105, -0.001485903159252705),\n",
       " (106, 0.23420641566065181),\n",
       " (107, -0.0070953398360357467),\n",
       " (108, 36.715295684780116),\n",
       " (109, -0.089066510923694914),\n",
       " (110, 0.037955067828277074),\n",
       " (111, 0.31937578212169909),\n",
       " (112, 0.46070890477221749),\n",
       " (113, 0.14109006854587103),\n",
       " (114, -0.66701731962769983),\n",
       " (115, -0.25903524468839056),\n",
       " (116, -0.00046060075501674902),\n",
       " (117, -0.015186823222487433),\n",
       " (118, 0.075476841008221718),\n",
       " (119, -0.0023610549786923229),\n",
       " (120, -0.15032823317625293),\n",
       " (121, 0.18557555812084664),\n",
       " (122, 0.63197922430890019),\n",
       " (123, -0.15025362451833552),\n",
       " (124, 0.018763881734815746),\n",
       " (125, -0.033809585111456486),\n",
       " (126, -0.4461040324600043)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## Linear Regression Model Without Regularization ##########\n",
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "# lm.coef_\n",
    "list(zip(X.columns.values,lm.coef_))\n",
    "# What are these numbers?\n",
    "# Gp - coefficient for every possible feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.6618816659782105"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X.columns\n",
    "lm.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (no regularization) = 0.233813676495\n"
     ]
    }
   ],
   "source": [
    "# make predictions and evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "preds = lm.predict(X_test)\n",
    "print('RMSE (no regularization) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Ridge reg.) = 0.164279068049\n"
     ]
    }
   ],
   "source": [
    "########## Ridge Regression Model ##########\n",
    "# ridge regression (alpha must be positive, larger means more regularization)\n",
    "from sklearn.linear_model import Ridge\n",
    "rreg = Ridge(alpha=0.1, normalize=True)\n",
    "rreg.fit(X_train, y_train)\n",
    "rreg.coef_\n",
    "preds = rreg.predict(X_test)\n",
    "print('RMSE (Ridge reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "# Is this model better? Why? \n",
    "# RMSE has lowered. Some variables have been removed that don't add information to the model.\n",
    "# If alpha is zero, this is equivalent to performing linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Alpha Value:  1.0\n",
      "RMSE (Ridge CV reg.) = 0.163129782343\n"
     ]
    }
   ],
   "source": [
    "# use RidgeCV to select best alpha\n",
    "# Very common procedure - ridge regression cross validation.\n",
    "from sklearn.linear_model import RidgeCV\n",
    "alpha_range = 10.**np.arange(-2, 3)\n",
    "rregcv = RidgeCV(normalize=True, scoring='neg_mean_squared_error', alphas=alpha_range)\n",
    "rregcv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal value of Alpha for Ridge Regression\n",
    "print('Optimal Alpha Value: ', rregcv.alpha_)\n",
    "\n",
    "# Print the RMSE for the ridge regression model\n",
    "preds = rregcv.predict(X_test)\n",
    "print ('RMSE (Ridge CV reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "# What is the range of alpha values we are searching over?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Lasso reg.) = 0.160039024044\n"
     ]
    }
   ],
   "source": [
    "########## Lasso Regression Model ##########\n",
    "# lasso (alpha must be positive, larger means more regularization)\n",
    "from sklearn.linear_model import Lasso\n",
    "las = Lasso(alpha=0.001, normalize=True)\n",
    "las.fit(X_train, y_train)\n",
    "las.coef_\n",
    "preds = las.predict(X_test)\n",
    "print('RMSE (Lasso reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "# GP - commentary - RMSE is 0.2 at alpha 0.01, so try 0.001 to get a better error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Lasso reg.) = 0.164502413721\n"
     ]
    }
   ],
   "source": [
    "# try a smaller alpha\n",
    "las = Lasso(alpha=0.0001, normalize=True)\n",
    "las.fit(X_train, y_train)\n",
    "las.coef_\n",
    "preds = las.predict(X_test)\n",
    "print('RMSE (Lasso reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "\n",
    "# GP - Search range and step size is important in feature selection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Alpha Value:  0.001\n",
      "RMSE (Lasso CV reg.) = 0.160039024044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# use LassoCV to select best alpha (tries 100 alphas by default)\n",
    "from sklearn.linear_model import LassoCV\n",
    "alpha_range = 10.**np.arange(-5, 3)\n",
    "lascv = LassoCV(normalize=True, alphas=alpha_range)\n",
    "\n",
    "# You can screw around with alpha_range - default range bottoms at 0.01.\n",
    "lascv.fit(X_train, y_train)\n",
    "print('Optimal Alpha Value: ',lascv.alpha_)\n",
    "lascv.coef_\n",
    "preds = lascv.predict(X_test)\n",
    "print('RMSE (Lasso CV reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Carry out Elastic net regularised regression\n",
    "\n",
    "### Lookup [Elastic Net](http://scikit-learn.org/stable/modules/linear_model.html#elastic-net) and complete the following.\n",
    "\n",
    "\n",
    "\n",
    "1. What is elastic net?\n",
    "2. How does it work?\n",
    "3. Run elastic net on the above dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nElastic Net is another great model to leverage in real life\\nRESEARCH\\n1. Another method of regularization. It combines some of the advantages of\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Elastic Net is a great tool to leverage in real life\n",
    "\n",
    "1. Elastic Net is another method of regularization. \n",
    "It combines some of the advantages of ridge regression, \n",
    "along with other advantages from lasso (i.e. few features\n",
    "are non zero). It's useful in data sets where multiple features are correlated\n",
    "with each other, and where there are more features than data points.\n",
    "2) ElasticNet works by introducing two regularizers - L1 and L2.\n",
    "This adds a quadratic element to the penalty.\n",
    "This is specified using a parameter called the l1_ratio (rho)\n",
    "3) \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (enet reg.) = 0.242253664949\n"
     ]
    }
   ],
   "source": [
    "# Setup the elastic net model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.7)\n",
    "enet.fit(X_train,y_train)\n",
    "preds2 = enet.predict(X_test)\n",
    "print('RMSE (enet reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds2)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alphas': array([  1.00000000e-03,   1.00000000e-02,   1.00000000e-01,\n",
      "         1.00000000e+00,   1.00000000e+01,   1.00000000e+02]), 'copy_X': True, 'cv': None, 'eps': 0.001, 'fit_intercept': True, 'l1_ratio': [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1], 'max_iter': 1000, 'n_alphas': 100, 'n_jobs': 1, 'normalize': False, 'positive': False, 'precompute': 'auto', 'random_state': None, 'selection': 'cyclic', 'tol': 0.0001, 'verbose': 0}\n",
      "Optimal Alpha Value:  0.01\n",
      "Optimal l1_ratio Value:  0.5\n",
      "RMSE (enetcv reg.) = 0.160399788864\n"
     ]
    }
   ],
   "source": [
    "# Setting up a CV version of the elastic net model\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "# Specifying parameter ranges\n",
    "alpha_rangeEN = 10.**np.arange(-3, 3)\n",
    "# l1_ratio_rangeEN = 10.**np.arange(-6,1) \n",
    "l1_ratio_rangeEN = [.1, .5, .7, .9, .95, .99, 1] # provides a better result.\n",
    "enetcv = ElasticNetCV(alphas=alpha_rangeEN,l1_ratio=l1_ratio_rangeEN)\n",
    "print(enetcv.get_params())\n",
    "enetcv.fit(X_train,y_train)\n",
    "print('Optimal Alpha Value: ',enetcv.alpha_)\n",
    "print('Optimal l1_ratio Value: ',enetcv.l1_ratio_)\n",
    "preds3 = enetcv.predict(X_test)\n",
    "print('RMSE (enetcv reg.) =', np.sqrt(metrics.mean_squared_error(y_test, preds3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 2: Carry out Regularised Regression\n",
    "\n",
    "1. Run all three forms of regularised regression on the Boston Housing DataSet\n",
    "2. What do the coefficients mean?\n",
    "3. What would you advise someone living in Boston to try and raise the value of their home?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso model:  -3.707 * LSTAT + 2.992 * RM + -1.757 * PTRATIO + -1.081 * DIS + -0.7 * NOX + 0.631 * B + 0.54 * CHAS + -0.236 * CRIM + 0.081 * ZN + -0.0 * INDUS + -0.0 * AGE + 0.0 * RAD + -0.0 * TAX\n",
      "Ridge model:  -2.723 * LSTAT + 2.68 * RM + -1.556 * PTRATIO + -1.368 * DIS + -0.752 * NOX + 0.743 * B + 0.726 * CHAS + -0.606 * CRIM + -0.528 * TAX + 0.516 * ZN + 0.462 * RAD + -0.443 * INDUS + -0.197 * AGE\n",
      "ElasticNet model:  0.049 * INDUS + -0.042 * CHAS + 0.0 * CRIM + 0.0 * ZN + -0.0 * NOX + 0.0 * RM + 0.0 * AGE + 0.0 * DIS + 0.0 * RAD + -0.0 * TAX + 0.0 * PTRATIO + 0.0 * B + -0.0 * LSTAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:39: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "\n",
    "# GP - importing the data\n",
    "\n",
    "boston = load_boston()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(boston[\"data\"]) # 13 features, 506 data points\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    "\n",
    "# GP - Lasso model\n",
    "\n",
    "lasso = Lasso(alpha=.3)\n",
    "lasso.fit(X, Y)\n",
    "\n",
    "# GP - LassoCV\n",
    "# from sklearn.linear_model import LassoCV\n",
    "# alpha_range = 10.**np.arange(-5, 3)\n",
    "# lascv = LassoCV(normalize=True, alphas=alpha_range)\n",
    "# lascv.fit(X, Y)\n",
    "# print('Optimal Alpha Value: ',lascv.alpha_)\n",
    "\n",
    "# GP - Ridge\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "rreg = Ridge(alpha=0.3, normalize=True)\n",
    "rreg.fit(X, Y)\n",
    "\n",
    "# GP - ElasticNet\n",
    "\n",
    "enet = ElasticNet(alpha=0.3, l1_ratio=0.1)\n",
    "enet.fit(X_train,y_train)\n",
    "\n",
    "#A helper method for pretty-printing linear models\n",
    "def pretty_print_linear(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)\n",
    "#Print all the model results\n",
    "\n",
    "print(\"Lasso model: \", pretty_print_linear(lasso.coef_, names, sort = True))\n",
    "print(\"Ridge model: \", pretty_print_linear(rreg.coef_, names, sort = True))\n",
    "print(\"ElasticNet model: \", pretty_print_linear(enet.coef_, names, sort = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "2)\n",
    "Definitions of features: https://archive.ics.uci.edu/ml/datasets/housing\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "3)\n",
    "Recommendations\n",
    "Can't change some things like LSTAT - got to think of levers\n",
    "Can build more rooms.\n",
    "Can chose post codes with rich neighbours, high pupil to teacher ratio.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
